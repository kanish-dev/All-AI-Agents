{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKbw7gdve+YUn9YLB6My8h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pbmcvkomocOI",
        "outputId": "806a06ac-792d-4ba3-b794-73df35ab5539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Up Our Playground"
      ],
      "metadata": {
        "id": "KxbmEjqhrCtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "state, info = env.reset()\n",
        "\n",
        "for _ in range(50):\n",
        "  env.render()\n",
        "\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  print(f\"State: {state.shape}, Action: {action}, Reward: {reward}\")\n",
        "\n",
        "  state = next_state\n",
        "\n",
        "  if terminated or truncated:\n",
        "    state, info = env.reset()\n",
        "\n",
        "  time.sleep(0.02)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbwqzgDxorad",
        "outputId": "84be63dd-05d2-4cab-a33f-9460c7610885"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 1, Reward: 1.0\n",
            "State: (4,), Action: 0, Reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Agent’s Brain with a Neural Network"
      ],
      "metadata": {
        "id": "q8Xg2B2Oq_PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network to approximate the Q-value function.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"\n",
        "        Initializes the network layers.\n",
        "        :param state_size: The number of features in the game state (e.g., 4 for CartPole).\n",
        "        :param action_size: The number of possible actions (e.g., 2 for CartPole).\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(), # activation function\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "        It takes a state and returns the Q-values for each action.\n",
        "        \"\"\"\n",
        "        return self.network(state)"
      ],
      "metadata": {
        "id": "BbqU8IL2rIsn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Creating the Agent’s Logic\n",
        "\n",
        "\n",
        "1.   Remembering Experiences\n",
        "2.   Deciding a Move\n",
        "3. Getting Smarter\n",
        "\n",
        "implementation the DQN Algorithm for creating the Agent’s logic:\n"
      ],
      "metadata": {
        "id": "dNJMvTmys5wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "UPDATE_EVERY = 4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DQNAgent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
        "        # initialize time step\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # if enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval() # set network to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train() # set network back to training mode\n",
        "\n",
        "        # epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # get max predicted Q-values for next states from the network\n",
        "        Q_targets_next = self.qnetwork_local(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # compute Q targets for current states\n",
        "        # target = reward + gamma * Q_next (if not done)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # get expected Q values from local model\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        # minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "    def __init__(self, action_size, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "_AVVb9SOs5L_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Training Loop"
      ],
      "metadata": {
        "id": "gk3Kxp36t6Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# Initialize Environment and Agent\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
        "\n",
        "# Training Hyperparameters\n",
        "n_episodes = 2000       # max number of training episodes\n",
        "max_t = 1000            # max number of timesteps per episode\n",
        "eps_start = 1.0         # starting value of epsilon\n",
        "eps_end = 0.01          # minimum value of epsilon\n",
        "eps_decay = 0.995       # multiplicative factor for decreasing epsilon\n",
        "\n",
        "def train():\n",
        "    scores = []                         # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)   # last 100 scores\n",
        "    eps = eps_start                     # initialize epsilon\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "\n",
        "        # decrease epsilon\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}', end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "\n",
        "        # check if the environment is solved\n",
        "        if np.mean(scores_window) >= 195.0:\n",
        "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "            # save the trained model's weights\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = train()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvvtjYNft54H",
        "outputId": "51e97a44-e61e-48e2-9432-9f5f1e256e4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 18.51\n",
            "Episode 200\tAverage Score: 106.01\n",
            "Episode 237\tAverage Score: 198.94\n",
            "Environment solved in 137 episodes!\tAverage Score: 198.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watching Our AI Agent Play the Game"
      ],
      "metadata": {
        "id": "N5s7P2_ZueeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Initialize the Environment and Agent\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# create an agent\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
        "\n",
        "# Step 2: Load the Trained Weights\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "# Step 3: Watch the Smart Agent Play\n",
        "num_episodes_to_watch = 10\n",
        "\n",
        "for i in range(num_episodes_to_watch):\n",
        "    # reset the environment to get the initial state\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    print(f\"--- Watching Episode {i+1} ---\")\n",
        "\n",
        "    while not done:\n",
        "        # render the environment\n",
        "        env.render()\n",
        "\n",
        "        # choose the best action using the trained network (epsilon=0)\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # perform the action in the environment\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # update the state\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        # add a small delay to make it watchable\n",
        "        time.sleep(0.02)\n",
        "\n",
        "    print(f\"Score for Episode {i+1}: {episode_reward}\")\n",
        "\n",
        "# close the environment window\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jn7stzqucSP",
        "outputId": "3ab9e2db-2155-45fc-80db-69d2376ec8ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Watching Episode 1 ---\n",
            "Score for Episode 1: 500.0\n",
            "--- Watching Episode 2 ---\n",
            "Score for Episode 2: 500.0\n",
            "--- Watching Episode 3 ---\n",
            "Score for Episode 3: 500.0\n",
            "--- Watching Episode 4 ---\n",
            "Score for Episode 4: 500.0\n",
            "--- Watching Episode 5 ---\n",
            "Score for Episode 5: 500.0\n",
            "--- Watching Episode 6 ---\n",
            "Score for Episode 6: 500.0\n",
            "--- Watching Episode 7 ---\n",
            "Score for Episode 7: 500.0\n",
            "--- Watching Episode 8 ---\n",
            "Score for Episode 8: 500.0\n",
            "--- Watching Episode 9 ---\n",
            "Score for Episode 9: 500.0\n",
            "--- Watching Episode 10 ---\n",
            "Score for Episode 10: 500.0\n"
          ]
        }
      ]
    }
  ]
}