{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOeiwxmOBl+sdABlQpDPRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanish-dev/All-AI-Agents/blob/main/Multi_Agent_System_using_Gemini_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiIlDWjNlEtm"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "def configure():\n",
        "  secret_name = \"GOOGLE_API_KEY\"\n",
        "\n",
        "  api_key = userdata.get(secret_name)\n",
        "  if not api_key:\n",
        "    raise ValueError(f\"Secret '{secret_name}' not found. please check\")\n",
        "\n",
        "  genai.configure(api_key=api_key)\n",
        "  return genai.GenerativeModel('models/gemini-2.5-flash')"
      ],
      "metadata": {
        "id": "5NfG29VomBRR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    print(f\"Model Name: {m.name}\")\n",
        "    print(f\"Description: {m.description}\")\n",
        "    print(f\"Input Modalities: {m.supported_generation_methods}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pqTBtdXc3CKp",
        "outputId": "a3a3a01f-c506-4642-e609-a7b8a4857b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Name: models/embedding-gecko-001\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedText', 'countTextTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-pro-preview-03-25\n",
            "Description: Gemini 2.5 Pro Preview 03-25\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-preview-05-20\n",
            "Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash\n",
            "Description: Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-lite-preview-06-17\n",
            "Description: Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-pro-preview-05-06\n",
            "Description: Preview release (May 6th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-pro-preview-06-05\n",
            "Description: Preview release (June 5th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-pro\n",
            "Description: Stable release (June 17th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-exp\n",
            "Description: Gemini 2.0 Flash Experimental\n",
            "Input Modalities: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash\n",
            "Description: Gemini 2.0 Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-001\n",
            "Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-exp-image-generation\n",
            "Description: Gemini 2.0 Flash (Image Generation) Experimental\n",
            "Input Modalities: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-lite-001\n",
            "Description: Stable version of Gemini 2.0 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-lite\n",
            "Description: Gemini 2.0 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-preview-image-generation\n",
            "Description: Gemini 2.0 Flash Preview Image Generation\n",
            "Input Modalities: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-lite-preview\n",
            "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-pro-exp\n",
            "Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-pro-exp-02-05\n",
            "Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-exp-1206\n",
            "Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp\n",
            "Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp-1219\n",
            "Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-preview-tts\n",
            "Description: Gemini 2.5 Flash Preview TTS\n",
            "Input Modalities: ['countTokens', 'generateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-pro-preview-tts\n",
            "Description: Gemini 2.5 Pro Preview TTS\n",
            "Input Modalities: ['countTokens', 'generateContent']\n",
            "--------------------\n",
            "Model Name: models/learnlm-2.0-flash-experimental\n",
            "Description: LearnLM 2.0 Flash Experimental\n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3-1b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3-4b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3-12b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3-27b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3n-e4b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemma-3n-e2b-it\n",
            "Description: \n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-flash-latest\n",
            "Description: Latest release of Gemini Flash\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-flash-lite-latest\n",
            "Description: Latest release of Gemini Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-pro-latest\n",
            "Description: Latest release of Gemini Pro\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-lite\n",
            "Description: Stable version of Gemini 2.5 Flash-Lite, released in July of 2025\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-image-preview\n",
            "Description: Gemini 2.5 Flash Preview Image\n",
            "Input Modalities: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-image\n",
            "Description: Gemini 2.5 Flash Preview Image\n",
            "Input Modalities: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-preview-09-2025\n",
            "Description: Gemini 2.5 Flash Preview Sep 2025\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-lite-preview-09-2025\n",
            "Description: Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite\n",
            "Input Modalities: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-robotics-er-1.5-preview\n",
            "Description: Gemini Robotics-ER 1.5 Preview\n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-computer-use-preview-10-2025\n",
            "Description: Gemini 2.5 Computer Use Preview 10-2025\n",
            "Input Modalities: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/embedding-001\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedContent']\n",
            "--------------------\n",
            "Model Name: models/text-embedding-004\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-embedding-exp-03-07\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-embedding-exp\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-embedding-001\n",
            "Description: Obtain a distributed representation of a text.\n",
            "Input Modalities: ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
            "--------------------\n",
            "Model Name: models/aqa\n",
            "Description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
            "Input Modalities: ['generateAnswer']\n",
            "--------------------\n",
            "Model Name: models/imagen-3.0-generate-002\n",
            "Description: Vertex served Imagen 3.0 002 model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/imagen-4.0-generate-preview-06-06\n",
            "Description: Vertex served Imagen 4.0 model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/imagen-4.0-ultra-generate-preview-06-06\n",
            "Description: Vertex served Imagen 4.0 ultra model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/imagen-4.0-generate-001\n",
            "Description: Vertex served Imagen 4.0 model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/imagen-4.0-ultra-generate-001\n",
            "Description: Vertex served Imagen 4.0 ultra model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/imagen-4.0-fast-generate-001\n",
            "Description: Vertex served Imagen 4.0 Fast model\n",
            "Input Modalities: ['predict']\n",
            "--------------------\n",
            "Model Name: models/veo-2.0-generate-001\n",
            "Description: Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.0-generate-preview\n",
            "Description: Veo 3 preview.\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.0-fast-generate-preview\n",
            "Description: Veo 3 fast preview.\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.0-generate-001\n",
            "Description: Veo 3\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.0-fast-generate-001\n",
            "Description: Veo 3 fast\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.1-generate-preview\n",
            "Description: Veo 3.1\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/veo-3.1-fast-generate-preview\n",
            "Description: Veo 3.1 fast\n",
            "Input Modalities: ['predictLongRunning']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.0-flash-live-001\n",
            "Description: Gemini 2.0 Flash 001\n",
            "Input Modalities: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-live-2.5-flash-preview\n",
            "Description: Gemini Live 2.5 Flash Preview\n",
            "Input Modalities: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-live-preview\n",
            "Description: Gemini 2.5 Flash Live Preview\n",
            "Input Modalities: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-native-audio-latest\n",
            "Description: Latest release of Gemini 2.5 Flash Native Audio\n",
            "Input Modalities: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model Name: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "Description: Gemini 2.5 Flash Native Audio Preview 09-2025\n",
            "Input Modalities: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AGENT 1 : THE PLANNER**"
      ],
      "metadata": {
        "id": "lqF6wz2DndcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner_agent(model, topic: str) ->list[str]:\n",
        "  print('Planner Agent: Creating a plan...')\n",
        "  prompt = f\"\"\"\n",
        "  You are an expert research planner. Yout task is to break down the following topic\n",
        "  into 3-5 specific, answerable questions. Return these questions as Python list of strings.\n",
        "\n",
        "  TOPIC : \"{topic}\"\n",
        "\n",
        "  Example output : [\"question 1\",\"question 2\",\"question 3\"]\n",
        "  \"\"\"\n",
        "  try :\n",
        "    response = model.generate_content(prompt)\n",
        "    plan_str = response.text.strip().replace('[','').replace(']','').replace('\"','')\n",
        "    plan = [q.strip() for q in plan_str.split(',') if q.strip()]\n",
        "\n",
        "    print('Plan created:')\n",
        "    for i,q in enumerate(plan,1):\n",
        "      print(f'{i}.{q}')\n",
        "    return plan\n",
        "  except Exception as e:\n",
        "    print(f'Error in Planner Agent: {e} ')\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "SbMDcwocnlkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AGENT 2: THE SEARCH AGENT**"
      ],
      "metadata": {
        "id": "ew2NYVOdrFSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_agent(model, question: str) -> str:\n",
        "    print(f\"Search Agent: Researching question: '{question}'...\")\n",
        "    try:\n",
        "        search_tool = genai.protos.Tool(\n",
        "            google_search_retrieval=genai.protos.GoogleSearchRetrieval()\n",
        "        )\n",
        "        prompt = f\"Provide a detailed answer to the following question: {question}\"\n",
        "        response = model.generate_content(prompt, tools=[search_tool])\n",
        "\n",
        "        print(\"   - Information found.\")\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Search Agent: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "lD3oLUYCrER9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AGENT 3 : THE SYNTHESISZER**"
      ],
      "metadata": {
        "id": "hwtL-wECt7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synthesizer_agent(model, topic: str, research_results: list) ->str:\n",
        "  print(\"Synthesizer Agent: Writing the final report...\")\n",
        "\n",
        "  reserach_notes = \"\"\n",
        "  for question, data in research_results:\n",
        "    reserach_notes += f\"### Question: {question}\\n### Research Data:\\n{data}\\n\\n===\\n\\n\"\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an expert research analyst. Your task is to synthesize the provided research notes\n",
        "  into a comprehensive, well-structured reporton the topic: \"{topic}\".\n",
        "\n",
        "  The report should contain an introduction, a Body that covers the key findings from the notes,\n",
        "  and a conclusion. Use the information from the research notes ONLY.\n",
        "\n",
        "  ## Research Notes ##\n",
        "  {reserach_notes}\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "  except Exception as e:\n",
        "    print(f\"Error in Synthesizer Agent: {e}\")\n",
        "    return \"Error: Could not genrate the final report.\""
      ],
      "metadata": {
        "id": "eSkCysC1uFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Conductor: The main() Orchestrator**"
      ],
      "metadata": {
        "id": "qCDX2kUdxt6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  try:\n",
        "    model = configure()\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    return\n",
        "  print(\"\\nHello! I am your AI Research Assistant.\")\n",
        "  topic = input(\" What topic would you like me to research today? \")\n",
        "\n",
        "  if not topic.strip():\n",
        "    print(\"A topic is requried to begin the research. Exiting.\")\n",
        "    return\n",
        "\n",
        "  print(f\"\\nStrarting research process for: '{topic}'\")\n",
        "\n",
        "  research_plan = planner_agent(model,topic)\n",
        "  if not research_plan:\n",
        "    print(\"Could not create a research plan. Exiting\")\n",
        "    return\n",
        "\n",
        "  research_results = []\n",
        "  for question in research_plan:\n",
        "    research_data = search_agent(model,question)\n",
        "    if research_data:\n",
        "      research_results.append((question, research_data))\n",
        "\n",
        "  if not research_results:\n",
        "    print(\"Could not find any information during the research. Exiting\")\n",
        "    return\n",
        "\n",
        "  final_report = synthesizer_agent(model,topic, research_results)\n",
        "\n",
        "  print(\"\\n\\n--- FINAL RESEARCH REPORT ---\")\n",
        "  print(f\"## TOPIC: {topic}\\n\")\n",
        "  print(final_report)\n",
        "  print(\"\\n\\n--- END OF REPORT ---\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "PioRmfkxxxi9",
        "outputId": "d817fb58-40ec-460c-8ddd-396de95810a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello! I am your AI Research Assistant.\n",
            " What topic would you like me to research today? Explain quantum computing in simple termsExplain quantum computing in simple terms\n",
            "\n",
            "Strarting research process for: 'Explain quantum computing in simple termsExplain quantum computing in simple terms'\n",
            "Planner Agent: Creating a plan...\n",
            "Plan created:\n",
            "1.```python\n",
            "\n",
            "  What are the core differences between a quantum computer and a regular computer\n",
            "2.in simple terms?\n",
            "3.What are the key quantum principles\n",
            "4.like superposition and entanglement\n",
            "5.that allow quantum computers to work?\n",
            "6.What kinds of problems are quantum computers expected to solve better than traditional computers?\n",
            "7.Is quantum computing a reality today\n",
            "8.and what are its current limitations?\n",
            "\n",
            "```\n",
            "Search Agent: Researching question: '```python\n",
            "\n",
            "  What are the core differences between a quantum computer and a regular computer'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 760.96ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'in simple terms?'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 734.66ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'What are the key quantum principles'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 739.64ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'like superposition and entanglement'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 734.74ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'that allow quantum computers to work?'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 709.66ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'What kinds of problems are quantum computers expected to solve better than traditional computers?'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 786.59ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'Is quantum computing a reality today'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 683.99ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Search Agent: Researching question: 'and what are its current limitations?\n",
            "\n",
            "```'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 683.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in Search Agent: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Search Grounding is not supported.\n",
            "Could not find any information during the research. Exiting\n"
          ]
        }
      ]
    }
  ]
}